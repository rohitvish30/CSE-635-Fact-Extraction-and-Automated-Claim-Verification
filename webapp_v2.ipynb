{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"webapp.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1wbbptX_8ERuKEdXUXUbvhHNZ_bfX4mk2","authorship_tag":"ABX9TyPqGx+4l0/jYfkH4WoOG68M"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"klMRDNKXv7EJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c4f73c09-40ca-4f5e-d3c2-a5bc99715f95","executionInfo":{"status":"ok","timestamp":1588622642579,"user_tz":240,"elapsed":306,"user":{"displayName":"Rohit Vishwakarma","photoUrl":"","userId":"15172601385992810991"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pZoRGlVrzPXU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":591},"outputId":"d21b23a3-0961-4286-b0f4-e6f49e388932","executionInfo":{"status":"ok","timestamp":1588622651019,"user_tz":240,"elapsed":8739,"user":{"displayName":"Rohit Vishwakarma","photoUrl":"","userId":"15172601385992810991"}}},"source":["#Installing the necessary files\n","!pip install wikipedia\n","!pip install rake-nltk\n","!python -m spacy download en_core_web_lg"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wikipedia in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n","Requirement already satisfied: rake-nltk in /usr/local/lib/python3.6/dist-packages (1.0.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rake-nltk) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->rake-nltk) (1.12.0)\n","Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4TFoy7Zev9XR","colab_type":"code","colab":{}},"source":["#importing other necessary libraries\n","import wikipedia as wki\n","from textblob import TextBlob as txb\n","import pandas as pd\n","import json\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import math\n","import statistics\n","from operator import itemgetter\n","import random\n","import gensim\n","from gensim import corpora,models,similarities\n","from rake_nltk import Metric, Rake\n","import spacy\n","import en_core_web_lg\n","import collections\n","from collections import OrderedDict\n","import operator\n","nlp = en_core_web_lg.load()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Udinzs-zeIr","colab_type":"code","colab":{}},"source":["# Get the WIKI dump of FEVER\n","table = []\n","nums = [f\"{i:03}\" for i in range(1,110)]\n","for i in nums:\n","    with open('/content/drive/My Drive/NLP/wiki-pages/wiki-'+i+'.jsonl') as f:\n","        for line in f:\n","          table.append(json.loads(line))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tMrDIQ14EI3","colab_type":"code","colab":{}},"source":["#Get the test Claims that needs to be labelled\n","test_claims = []\n","with open('/content/drive/My Drive/NLP/shared_task_dev_public.jsonl', 'r') as f:\n","    for line in f:\n","        test_claims.append(json.loads(line))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Ix33eIuj3nj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"7351e2e4-2cda-446a-dc4c-fb1c0c14b186","executionInfo":{"status":"ok","timestamp":1588623126149,"user_tz":240,"elapsed":483845,"user":{"displayName":"Rohit Vishwakarma","photoUrl":"","userId":"15172601385992810991"}}},"source":["# This libraries will help to deploy model to web app\n","!pip install flask-ngrok\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, request, jsonify, render_template\n","# from flask_debug import Debug\n","import pickle"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.6/dist-packages (0.0.25)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.9)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BYw99B85Cw7e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":457},"outputId":"fca3f874-e255-47de-941f-f467f0266e9b","executionInfo":{"status":"ok","timestamp":1588623131721,"user_tz":240,"elapsed":489410,"user":{"displayName":"Rohit Vishwakarma","photoUrl":"","userId":"15172601385992810991"}}},"source":["!pip install transformers\n","!pip install jsonlines"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n","Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wwvePr2TC7CO","colab_type":"code","colab":{}},"source":["import json\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","import torch\n","import transformers as ppb\n","import warnings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1P8KtqwDF6f","colab_type":"code","colab":{}},"source":["# Load pre-trained model and tokenizer \n","model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n","tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","model = model_class.from_pretrained(pretrained_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wvkXnQL40ct","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"outputId":"fdf8a2e2-8e60-4fea-cbf2-ab463909f43c"},"source":["#Code for running the Web app\n","import numpy as np\n","import jsonlines\n","import re\n","# app = Flask(__name__)\n","app = Flask(__name__, root_path='/content/drive/My Drive/')\n","run_with_ngrok(app)\n","def remove_string_special_characters(s):\n","    stripped =  re.sub('[^\\w\\s]', '', s)\n","    stripped = re.sub('_', '', stripped)\n","    stripped = re.sub('\\s+', ' ', stripped)\n","    stripped = stripped.strip()\n","    return stripped\n","\n","def segregateSourceSentId(sorted_dict,temp2):\n","  newList=[]\n","  linesList=[]\n","  eachsource=[]\n","  if sorted_dict:\n","    for keyString in sorted_dict.keys():\n","      temp=[]\n","      source,id=keyString.split(\"\\t\") \n","      eachsource.append(str(source))\n","      temp.append(str(source))\n","      temp.append(id)\n","      newList.append(temp)\n","      linesList.append(str(temp2.get(keyString))+\"\\n\")       \n","  return eachsource,newList,linesList\n","\n","def similarityCheck(claim,eachLine):\n","  token1=nlp(claim)\n","  token2=nlp(eachLine)\n","  return token1.similarity(token2)\n","\n","def splitByEachLine(result,claim,lines):\n","  sourceLineIdScore={}\n","  sourceLineIdsentence={}\n","  allLine=lines.split('\\n')\n","  i=0\n","  for eachLine in allLine:\n","    cleanSentence=remove_string_special_characters(eachLine)\n","    score=similarityCheck(claim,eachLine)\n","    key=(str(result)+\"\\t\"+str(i))\n","    sourceLineIdScore[key]=score\n","    sourceLineIdsentence[key]=eachLine\n","    i+=1\n","  #print(sentenceWithScore)\n","  return sourceLineIdScore,sourceLineIdsentence\n","\n","#define example and feature classes\n","class InputExample(object):\n","  def __init__(self, claim, ev, label=None):\n","    self.claim = claim\n","    self.ev = ev\n","    self.label = label\n","\n","class InputFeatures(object):\n","    def __init__(self, claim_ids, ev_ids, label_id):\n","        self.claim_ids = claim_ids\n","        self.ev_ids = ev_ids\n","        self.label_id = label_id\n","\n","def data_to_examples(line):\n","  examples = []\n","  #data_file=json.dumps(data1)\n","  #data = jsonlines.open(data_file)\n","  #labels = jsonlines.open('train_output.jsonl')\n","  #for line,line2 in zip(data,labels):\n","  #for index in range(845):\n","  #line = data.read()\n","# label = line2['label']\n","  claim = line['claim']\n","  ev=[]\n","  if line['evidences'] != 'null':\n","    for item in line['evidences']:\n","      #remove nonsense from sents\n","      clean = re.sub('\\n+',' ',item)\n","      clean = re.sub('\\s+',' ',clean)\n","      clean = re.sub('[a-zA-Z0-9]*\\t[a-zA-Z0-9]*','',clean)\n","      clean = re.sub('-...-','',clean)\n","      if len(clean.split()) <= 512:\n","        ev.append(clean)\n","  else:\n","    ev.append('null')\n","  examples.append(InputExample(claim,ev))#,label))\n","  return examples\n","\n","def example_to_features(example):\n","  #tokenize and add [CLS]/[SEP] tokens\n","  claim_tokens = tokenizer.encode(example.claim, add_special_tokens=True)\n","  ev_tokens = []\n","  if example.ev ==[]:\n","    ev_tokens.append([101])\n","  for item in example.ev:\n","    ev_tokens.append(tokenizer.encode(item,add_special_tokens=True))\n","  \n","  return InputFeatures(claim_tokens,ev_tokens,example.label)\n","\n","\n","loaded_model = pickle.load(open('/content/drive/My Drive/NLP/new_trained_model_15k.sav', 'rb'))      \n","\n","\n","#print(type(lines))\n","\n","@app.route('/')\n","def home():\n","  return render_template('index.html')\n","\n","@app.route('/test',methods=['POST'])\n","def test():\n","  # claim = request.form[\"claim_text_area\"]\n","  \n","  # claim = [request.form.values()]\n","\n","  claim=request.form['claim']\n","  rake = Rake(\"/content/drive/My Drive/NLP/stopwords.txt\")\n","  rake = Rake(min_length=2, max_length=5)\n","  result={}\n","  #counter += 1\n","  phrases=[]   \n","  #temp_str=claims['claim']\n","  #rake.extract_keywords_from_text(temp_str)\n","  rake.extract_keywords_from_text(claim)\n","  phrases=rake.get_ranked_phrases()\n","  phrases.append(claim)\n","  temp_results=[]\n","  sourceLineIdScoreResultWise=[]\n","  temp1={}\n","  temp2={}\n","  sourceLineIdsentenceResultWise=[]\n","  if(len(phrases) != 0):\n","    for each in phrases:\n","      temp_results.extend(wki.search(each,results=1))\n","    for result in temp_results:\n","      if result is not 'null':\n","        temp = result.replace(\" \", \"_\")\n","        try:\n","          lines = str(next(item['lines'] for item in table if item[\"id\"] == temp))\n","          text_sents = sent_tokenize(lines)\n","          sourceLineIdScore,sourceLineIdsentence=splitByEachLine(result,claim,lines)\n","          sourceLineIdScoreResultWise.append(sourceLineIdScore)\n","          sourceLineIdsentenceResultWise.append(sourceLineIdsentence)\n","        except StopIteration:\n","          continue\n","        \n","        for eachDict in sourceLineIdScoreResultWise:\n","          temp1.update(eachDict)\n","        for eachDict in sourceLineIdsentenceResultWise:\n","          temp2.update(eachDict)\n","  d_descending = sorted(temp1.items(), key=operator.itemgetter(1),reverse=True)[:3] #sort by values i.e top 3 values in descending order \n","  sorted_dict = dict(OrderedDict(d_descending))\n","  eachSource,sourcess,evidencess=segregateSourceSentId(sorted_dict,temp2)\n","  output_row = {}\n","  output_row['claim'] = claim\n","  output_row['sources'] = sourcess\n","  output_row['evidences'] = evidencess\n","  # json.dump(output_row, outfile);\n","  # outfile.write(\"\\n\");\n","  # print(counter)\n","  #return output_row\n","\n","  test_ex = data_to_examples(output_row)\n","\n","  feat_list = []\n","  for example in test_ex:\n","    features = example_to_features(example)\n","    feat_list.append(features)\n","\n","\n","  #####BOTTLENECK STEP\n","#create BERT representations for each example\n","  max_len = 432\n","  BERT_examples = []\n","  round = 0\n","  for item in feat_list:\n","    #padding\n","    padded_claim = np.array(item.claim_ids + [0]*(max_len-len(item.claim_ids)))\n","    padded_claim = np.reshape(padded_claim,(1,max_len))\n","    padded_ev = np.array([sent + [0]*(max_len-len(sent)) for sent in item.ev_ids])\n","    \n","    #mask padded tokens\n","    claim_mask = np.where(padded_claim != 0,1,0)\n","    ev_mask = np.where(padded_ev != 0,1,0)\n","\n","    #create matrix: each row represents one sentence\n","    padded_total = np.concatenate((padded_claim,padded_ev),axis=0)\n","    mask_total = np.concatenate((claim_mask,ev_mask))\n","\n","    #convert to tensors\n","    input_ids = torch.tensor(padded_total)\n","    attention_mask = torch.tensor(mask_total)\n","\n","    #get BERT embeddings\n","    with torch.no_grad():\n","      last_hidden_states = model(input_ids, attention_mask=attention_mask)\n","\n","    #extract [CLS] embeddings and labels for input to classifier\n","    features = last_hidden_states[0][:,0,:].numpy()\n","    concatenated = features.flatten()\n","    BERT_examples.append(list(concatenated))\n","    round += 1\n","  \n","  #pad each example for uniform input size\n","  max_len = 109824\n","\n","  padded_BERT = []\n","  for example in BERT_examples:\n","    padded_BERT.append(example + [0]*(max_len-len(example)))\n","  \n","  test_inputs = np.array([np.array(ex) for ex in padded_BERT])\n","  \n","  predictions = loaded_model.predict(test_inputs)\n","  answer=\"The claim \\\"\"+claim+\"\\\"\"\n","  label_color=\"red\"\n","  if predictions[0]==\"SUPPORTS\":\n","    result1=\"IS TRUE (SUPPORTS)\"\n","  elif predictions[0]==\"REFUTES\":\n","    result1=\"IS FALSE (REFUTES)\"\n","  else:\n","    result1=\"Cannot be determined\"\n","\n","  # if eachSource:\n","  #   for i in range(len(eachSource)):\n","  #     if(i==0):\n","  #       source_text0=eachSource[0]\n","  #     elif(i==1):\n","  #       source_text1=eachSource[1]\n","  #     elif(i==2):\n","  #       source_text2=eachSource[2]  \n","\n","  #sourceEvidence = remove_string_special_characters(s)\n","  #tohit=np.array(claim)\n","  return render_template('index.html', result_text=\"{}\".format(result1),claim_text='{}'.format(answer))\n","  # return render_template('index.html',source_text0=\"{}\".format(source_text0),source_text1=\"{}\".format(source_text1),source_text2=\"{}\".format(source_text2), result_text=\"{}\".format(result1),claim_text='{}'.format(answer))\n","\n","if __name__ == \"__main__\":\n","  # Debug(app)\n","  app.run()"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://51c753c6.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [04/May/2020 21:58:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [04/May/2020 21:58:22] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [04/May/2020 21:58:36] \"\u001b[37mPOST /test HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"1e7Hn0ZE423X","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}